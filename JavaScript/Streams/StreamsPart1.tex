\documentclass[12pt,letterpaper]{article}
\usepackage[utf8]{inputenc}
\usepackage[letterpaper, margin=1in, bottom=2.5in, top=2.2in]{geometry}
\usepackage{fancyhdr}
\setlength{\headheight}{15pt}
\setlength{\footskip}{72pt}

\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{anyfontsize}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{listings}
\usepackage{sourcesanspro}
\renewcommand{\familydefault}{\sfdefault}
\usepackage{fontawesome5}
\usepackage{titlesec}
\usepackage{setspace}
\usepackage{hyperref}
\usepackage{mdframed}
\usepackage{qrcode} % Añadir paquete para generar códigos QR
\usetikzlibrary{calc,shapes,positioning}

\AtBeginDocument{\color{primaryColor}}

% Colors updated with JavaScript brand colors
\definecolor{bgColor}{RGB}{13, 17, 23}  % Dark background similar to JS docs
\definecolor{primaryColor}{RGB}{255, 255, 255}  % White text for better contrast
\definecolor{accentColor}{RGB}{255, 220, 60}  % Brighter JavaScript Yellow for better contrast
\definecolor{jsSecondary}{RGB}{180, 180, 180}  % Light gray as secondary color for better contrast
\definecolor{secondaryColor}{RGB}{230, 235, 240}  % Almost white gray for better contrast
\definecolor{terminalBg}{RGB}{22, 22, 22}  % Terminal background
\definecolor{terminalFrame}{RGB}{40, 40, 40}  % Terminal frame
\definecolor{lineNumberColor}{RGB}{100, 100, 100}  % Line number color
\definecolor{dividerColor}{RGB}{120, 130, 150}  % Brighter divider color for better visibility

% Colores para código 
\definecolor{codeTextColor}{RGB}{255, 255, 255}  % Blanco puro para el texto básico
\definecolor{codeKeywordColor}{RGB}{97, 175, 239}  % Azul claro para palabras clave
\definecolor{codeCommentColor}{RGB}{121, 192, 120}  % Verde para comentarios
\definecolor{codeStringColor}{RGB}{255, 220, 60}  % Amarillo JS más brillante para strings
\definecolor{codeMethodColor}{RGB}{255, 160, 122}  % Naranja claro para métodos
\definecolor{codeFunctionColor}{RGB}{230, 180, 80}  % Tono amarillo-dorado para funciones
\definecolor{codeNumberColor}{RGB}{180, 230, 180}  % Verde claro para números

\pagecolor{bgColor}

\hypersetup{
    colorlinks=true,
    linkcolor=accentColor,
    filecolor=accentColor,
    urlcolor=accentColor,
}

% Definir lenguaje JavaScript para listings
\lstdefinelanguage{JavaScript}{
  keywords={break, case, catch, continue, debugger, default, delete, do, else, false, finally, for, function, if, in, instanceof, new, null, return, switch, this, throw, true, try, typeof, var, void, while, with, let, const, class, export, import, yield, async, await},
  sensitive=true,
  comment=[l]{//},
  morecomment=[s]{/*}{*/},
  morestring=[b]',
  morestring=[b]"
}

% Configuración para listings con JavaScript como lenguaje predeterminado
\lstset{
  language=JavaScript,
  basicstyle=\small\ttfamily\bfseries\color{codeTextColor},
  backgroundcolor=\color{terminalBg},
  commentstyle=\color{codeCommentColor},
  keywordstyle=\color{codeKeywordColor},
  stringstyle=\color{codeStringColor},
  numberstyle=\color{lineNumberColor},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=2,
  showstringspaces=false,
  frame=none,
  xleftmargin=15pt,
  xrightmargin=0pt,
  aboveskip=5pt,
  belowskip=5pt,
  numbers=left,
  numbersep=8pt,
  extendedchars=true,
  keepspaces=true,
  columns=flexible,
  lineskip=2pt,
  emph={[2]require,createReadStream,createWriteStream,pipe,on,once,createGzip,pipeline,Transform,Readable,Writable,Duplex},
  emphstyle={[2]\color{codeFunctionColor}}
}

\newenvironment{macterminal}{%
    \begin{mdframed}[
        linecolor=terminalFrame,
        backgroundcolor=terminalBg,
        roundcorner=5pt,
        skipabove=5pt,
        skipbelow=5pt,
        linewidth=1pt,
        innertopmargin=5pt,
        frametitle={%
            \tikz[baseline=(current bounding box.east), outer sep=0pt]{
                \fill[red!80!black] (0,0) circle (5pt);
                \fill[yellow!80!black] (0.7,0) circle (5pt);
                \fill[green!70!black] (1.4,0) circle (5pt);
            }
        },
        frametitlealignment=\raggedright,
        frametitleaboveskip=8pt,
        frametitlebelowskip=0pt,
    ]
}{%
    \end{mdframed}%
}

\newcommand{\verspace}{\vspace{5pt}}

\titleformat{\section}
  {\LARGE\bfseries\color{primaryColor}}
  {\thesection. }
  {0pt}
  {}
  []

\titleformat{\subsection}
  {\Large\bfseries\color{accentColor}}
  {\thesubsection. }
  {0pt}
  {}
  []

\titleformat{\subsubsection}
  {\large\bfseries\color{primaryColor}}
  {\thesubsubsection. }
  {0pt}
  {}
  []

\titlespacing*{\section}{0pt}{18pt}{8pt}
\titlespacing*{\subsection}{0pt}{12pt}{5pt}
\titlespacing*{\subsubsection}{0pt}{8pt}{3pt}

\pagestyle{fancy}
\fancyhf{}

\fancyhead[L]{
    \begin{tikzpicture}[remember picture, overlay]
        \fill[accentColor, rounded corners=3pt] (0,0) rectangle (2.2cm,0.7cm);
        \node[text=bgColor, font=\bfseries] at (1.1cm,0.35cm) {NODE.JS};
        \fill[secondaryColor] (2.35cm,0.1cm) rectangle (2.40cm,0.6cm);
        \node[text=primaryColor, font=\bfseries, anchor=west] at (2.35cm,0.35cm) {STREAMS};
    \end{tikzpicture}
}

\renewcommand{\headrulewidth}{0pt}

\renewcommand{\footrule}{
  \vspace{0.5cm}
  \noindent\makebox[\linewidth]{\color{dividerColor}\rule{\linewidth}{0.2pt}}
  \vspace{0.5cm}
}
\renewcommand{\footrulewidth}{0.2pt}
\renewcommand{\footruleskip}{1cm}

\fancyfoot[C]{
    \vspace*{0.1cm}
    \noindent
    \begin{minipage}{\textwidth}
        \begin{flushleft}
            % Foto de perfil
            \raisebox{0.7cm}{
            \begin{tikzpicture}[baseline]
                \path[fill=bgColor] (0,0) circle (0.6cm);
                \clip (0,0) circle (0.6cm);
                \node at (0,0) {
                    \includegraphics[width=1.2cm,height=1.2cm]{profile-image.jpeg}
                };
            \end{tikzpicture}
            }
            % Información de perfil
            \begin{minipage}[b]{0.5\textwidth}
                {\large\bfseries\color{primaryColor}Alejandro Sánchez Yalí}
                \par\vspace{1pt}
                {\small\color{secondaryColor}Software Developer | AI \& Blockchain Enthusiast}
                \par\vspace{1pt}
                {\small\color{accentColor}\faGlobe\hspace{5pt}\color{secondaryColor}www.asanchezyali.com}
            \end{minipage}
        \end{flushleft}
        \vspace{6pt}
    \end{minipage}
}

\fancypagestyle{plain}{
    \fancyhf{}
    \renewcommand{\headrulewidth}{0pt}
    \renewcommand{\footrulewidth}{0pt}
}

\renewcommand{\labelitemi}{\textcolor{accentColor}{$\bullet$}}
\renewcommand{\labelitemii}{\textcolor{secondaryColor}{$\circ$}}

\usepackage{relsize}
\AtBeginDocument{\relsize{1}}

\newcommand{\languagetag}[1]{
    \begin{tikzpicture}[baseline]
        \node[fill=accentColor, text=bgColor, rounded corners=5pt, inner sep=7pt] {
            {\normalsize\textbf{#1}}
        };
    \end{tikzpicture}
}

% Comando para QR elegante (para la página de título)
\newcommand{\elegantqr}[2]{
    \qrcode[height=2.5cm]{#1}
    \\[0.1cm]
    {\hspace{0.2cm}\color{primaryColor}\small #2\par}
}

\newcommand{\titlepagecontents}{%
    \vspace*{3cm}
    % Asegurar que todos los elementos estén alineados a la izquierda
    \begin{flushleft}
    \languagetag{Node.js}\\[0.4cm]
    {\fontsize{48}{52}\bfseries\color{primaryColor}Streams in \color{accentColor}Node.js\par}
    \vspace{0.3cm}
    {\fontsize{18}{52}\color{secondaryColor}Part 1: Introduction \& Memory Efficiency\par} 
    \vspace{0.3cm}
    {\color{secondaryColor}\today\par}
    \vspace{2cm}
    % El QR se alineará a la izquierda aunque internamente esté centrado
    \elegantqr{https://github.com/asanchezyali/social-media-posts/tree/main/Python/Decorators}{Source Code}
    \end{flushleft}
}

\newcommand{\finalpagecontents}{%
    \vspace*{4cm}
    \begin{center}
        \begin{tikzpicture}
            % Title text
            \node[text width=14cm, align=center] at (0,0) {
                {\fontsize{48}{52}\bfseries\color{primaryColor}Ready to \color{accentColor}Transform\\\color{primaryColor}Your Data Flow?\par}
            };
            
            % Add vertical space
            \node at (0,-3) {};
            
            % Simplified message text with larger font
            \node[text width=14cm, align=center] at (0,-5) {
                {\fontsize{24}{30}\color{secondaryColor}You've discovered the power of Node.js Streams.\\\vspace{1cm}
                \color{accentColor}Stay tuned for Part 2...\\\vspace{0.4cm}
                \color{primaryColor}Where we'll unlock the full potential\\\vspace{0.2cm}of your data pipelines!\par}
            };
        \end{tikzpicture}
    \end{center}
}

\begin{document}
\setlength{\parindent}{0pt}
\color{primaryColor}

\begin{titlepage}
    \titlepagecontents
\end{titlepage}

\section{What are Node.js Streams?}

Streams in Node.js are abstract interfaces that implement the EventEmitter pattern for handling flowing data. Unlike traditional data processing methods, streams operate on data sequentially in chunks instead of loading entire datasets into memory.

This approach offers three key advantages:

\begin{itemize}
    \item \textbf{\textcolor{accentColor}{Memory Efficiency:}} Processing data in small chunks reduces memory consumption, making streams ideal for large files.
    
    \item \textbf{\textcolor{accentColor}{Time Efficiency:}} Operations begin as soon as the first chunks arrive, without waiting for complete data transmission.
    
    \item \textbf{\textcolor{accentColor}{Composability:}} Streams can be connected through piping, enabling complex data processing pipelines.
\end{itemize}

This architecture naturally aligns with how data flows in many systems, from file I/O to network communications.

\subsection{Memory Efficiency: A Practical Example}

We'll examine a comprehensive example demonstrating the memory efficiency of streams when processing large files. The example is split into logical parts for better understanding.

\subsubsection{Part 1: Setup and Utility Functions}

Let's start by setting up the environment and defining utility functions for our comparison:

\begin{macterminal}
\begin{lstlisting}
const fs = require('fs');
const { promisify } = require('util');
const readFile = promisify(fs.readFile);

// Simple results storage
const results = { stream: null, noStream: null };

// Function to measure memory usage in MB (using RSS for more accurate measurement)
function getMemoryUsage() {
  return Math.round(process.memoryUsage().rss / 1024 / 1024 * 100) / 100;
}
\end{lstlisting}
\end{macterminal}

This first part sets up our environment by:
\begin{itemize}
    \item Importing necessary Node.js modules
    \item Creating a storage object to hold our comparison results
    \item Defining a utility function to measure memory usage in megabytes
\end{itemize}

\subsubsection{Part 2: Processing Without Streams}

Next, we implement the traditional approach that loads the entire file into memory:

\begin{macterminal}
\begin{lstlisting}
// Processing WITHOUT streams - loads entire file into memory
async function processWithoutStreams(filePath) {
  const initialMemory = getMemoryUsage();
  console.log(`Memory before loading file: ${initialMemory} MB`);
  
  // Read the entire file into memory
  const data = await readFile(filePath);
  
  const afterLoadMemory = getMemoryUsage();
  console.log(`Memory after loading file: ${afterLoadMemory} MB`);
  console.log(`File size: ${(data.length / 1024 / 1024).toFixed(2)} MB`);
  
  // Process the data (counting lines)
  const lines = data.toString().split('\n');
  
  const finalMemory = getMemoryUsage();
  console.log(`Lines processed: ${lines.length}`);
  console.log(`Final memory usage: ${finalMemory} MB`);
  
  results.noStream = { 
    initial: initialMemory,
    afterLoad: afterLoadMemory,
    max: Math.max(initialMemory, afterLoadMemory, finalMemory), 
    final: finalMemory 
  };
}
\end{lstlisting}
\end{macterminal}

This function demonstrates the traditional approach with these key steps:
\begin{itemize}
    \item It measures initial memory usage before any operations
    \item It loads the entire file into memory at once using \textbf{\textcolor{accentColor}{readFile}}
    \item It measures memory usage after loading the file
    \item It performs a simple operation (counting lines) on the entire file
    \item It records final memory usage and stores statistics for comparison
\end{itemize}

The main disadvantage of this approach is that the entire file must be loaded into memory before processing can begin, which can be problematic for large files.

\subsubsection{Part 3: Processing With Streams}

Now we implement the stream-based approach that processes data in chunks:

\begin{macterminal}
\begin{lstlisting}
// Processing WITH streams - processes the file in chunks
function processWithStreams(filePath) {
  return new Promise((resolve) => {
    const initialMemory = getMemoryUsage();
    console.log(`Memory before starting stream: ${initialMemory} MB`);
    
    let linesCount = 0;
    let maxMemory = initialMemory;
    let incompleteLine = '';
    let chunkCount = 0;
    
    const readStream = fs.createReadStream(filePath, {
      highWaterMark: 16 * 1024 // 16KB chunks
    });
    
    readStream.on('data', (chunk) => {
      chunkCount++;
      
      // Process the chunk
      const data = incompleteLine + chunk.toString();
      const lines = data.split('\n');
      incompleteLine = lines.pop();
      linesCount += lines.length;
      
      // Track memory usage
      const currentMemory = getMemoryUsage();
      maxMemory = Math.max(maxMemory, currentMemory);
    });
    
    readStream.on('end', () => {
      if (incompleteLine) linesCount++;
      const finalMemory = getMemoryUsage();
      
      console.log(`Lines processed: ${linesCount}`);
      console.log(`Total chunks: ${chunkCount}`);
      console.log(`Maximum memory used: ${maxMemory} MB`);
      console.log(`Final memory usage: ${finalMemory} MB`);
      
      results.stream = { 
        initial: initialMemory,
        max: maxMemory, 
        final: finalMemory 
      };
      resolve();
    });

    readStream.on('error', (err) => {
      console.error('Error reading file:', err);
      resolve();
    });
  });
}
\end{lstlisting}
\end{macterminal}

The stream-based approach works differently:
\begin{itemize}
    \item It creates a read stream with a small buffer size (16KB chunks)
    \item It processes data incrementally as it arrives in chunks
    \item It handles line splitting across chunk boundaries with the \textbf{\textcolor{accentColor}{incompleteLine}} variable
    \item It tracks maximum and final memory usage throughout processing
    \item It returns a promise that resolves when processing completes
\end{itemize}

Key aspects of this implementation:
\begin{itemize}
    \item The \textbf{\textcolor{accentColor}{highWaterMark}} option controls the buffer size
    \item The stream emits «data» events for each chunk
    \item We handle incomplete lines that might span across chunks
    \item Memory usage is tracked continuously during processing
\end{itemize}

\subsubsection{Part 4: Comparison Function and Results}

Finally, we implement a function to run both approaches and compare their performance:

\begin{macterminal}
\begin{lstlisting}
// Main comparison function
async function compareMemoryUsage(filePath) {
  // Get file size for display
  try {
    const stats = fs.statSync(filePath);
    const fileSizeMB = (stats.size / (1024 * 1024)).toFixed(2);
    console.log(`Processing file: ${filePath} (${fileSizeMB} MB)`);
  } catch (err) {
    console.log(`Processing file: ${filePath}`);
  }
  
  // 1. Run with streams
  console.log('WITH STREAMS:');
  await processWithStreams(filePath);
  
  // Wait for garbage collection
  console.log('\nWaiting for garbage collection...');
  await new Promise(resolve => setTimeout(resolve, 2000));
  
  // 2. Run without streams
  console.log('WITHOUT STREAMS:');
  await processWithoutStreams(filePath);
  
  // Print comparison
  console.log('\n============= MEMORY USAGE COMPARISON =============');
  console.log('| Method        | Maximum Memory | Final Memory  |');
  console.log('|---------------|---------------|---------------|');
  console.log(`| WITHOUT STREAM | ${results.noStream.max.toFixed(2).padStart(7)} MB    | ${results.noStream.final.toFixed(2).padStart(7)} MB    |`);
  console.log(`| WITH STREAM    | ${results.stream.max.toFixed(2).padStart(7)} MB    | ${results.stream.final.toFixed(2).padStart(7)} MB    |`);
  console.log('=================================================');
  
  // Memory increase from initial to after loading the file (non-stream method)
  const memoryIncrease = results.noStream.afterLoad - results.noStream.initial;
  console.log(`Loading the entire file increased memory by: ${memoryIncrease.toFixed(2)} MB`);
  
  // Improvement percentage
  const improvement = ((results.noStream.max - results.stream.max) / results.noStream.max * 100).toFixed(2);
  console.log(`Streams used ${improvement}% less maximum memory!`);
  
  // Tutorial explanation
  console.log('CONCLUSION:');
  console.log('   - Streams process data in small chunks (one at a time)');
  console.log('   - Without streams, the entire file is loaded into memory');
  console.log('   - For large files, streams significantly reduce memory usage');
}

// Run the comparison
const filePath = process.argv[2] || 'file.txt';
compareMemoryUsage(filePath);
\end{lstlisting}
\end{macterminal}

The comparison function provides a comprehensive analysis:
\begin{itemize}
    \item It runs both approaches sequentially
    \item It displays file information before processing
    \item It adds a delay between runs to allow for garbage collection
    \item It presents the results in a well-formatted comparison table
    \item It calculates memory efficiency improvements as a percentage
    \item It explains the conclusion of the comparison
\end{itemize}

\subsubsection{Comparison Results}

When run with a 408MB file, the results are compelling:

\begin{macterminal}
\begin{lstlisting}
Processing file: file.txt (408.02 MB)

WITH STREAMS:
Memory before starting stream: 36.64 MB
Lines processed: 1425071
Total chunks: 26114
Maximum memory used: 58.98 MB
Final memory usage: 56.61 MB

Waiting for garbage collection...

WITHOUT STREAMS:
Memory before loading file: 57.02 MB
Memory after loading file: 459.81 MB
File size: 408.02 MB
Lines processed: 1425071
Final memory usage: 843.95 MB

============= MEMORY USAGE COMPARISON =============
| Method        | Maximum Memory | Final Memory  |
|---------------|---------------|---------------|
| WITHOUT STREAM |  843.95 MB    |  843.95 MB    |
| WITH STREAM    |   58.98 MB    |   56.61 MB    |
=================================================

Loading the entire file increased memory by: 402.79 MB
Streams used 93.01% less maximum memory!

CONCLUSION:
   - Streams process data in small chunks (one at a time)
   - Without streams, the entire file is loaded into memory
   - For large files, streams significantly reduce memory usage
\end{lstlisting}
\end{macterminal}

The results clearly demonstrate the memory efficiency of streams:
\begin{itemize}
    \item The non-stream approach used 843.95 MB of memory
    \item The stream-based approach used only 58.98 MB of memory
    \item This represents a 93.01\% reduction in memory usage
    \item Both approaches processed the same number of lines (1,425,071)
\end{itemize}

This example provides compelling evidence for using streams when processing large files in Node.js applications.

\section{Conclusions}

\subsection{Memory Efficiency is Critical for Scalable Applications}

Node.js streams provide a powerful solution to memory management challenges in modern applications. As demonstrated in our comparative analysis, processing large datasets with streams can reduce memory consumption by over 90\%. This efficiency becomes increasingly important as applications scale and data volumes grow exponentially. By implementing streams in memory-intensive operations, developers can build more resilient systems that maintain performance even under heavy loads.

\subsection{Chunked Processing Enables Real-Time Capabilities}

The ability to process data in small, manageable chunks creates opportunities for real-time data handling that would be impossible with traditional methods. Rather than waiting for complete datasets to load, stream-based applications can begin processing immediately as data becomes available. This characteristic makes streams particularly valuable for applications requiring low-latency responses, such as real-time analytics, live data visualization, and responsive user interfaces.

\subsection{Streams Represent a Shift in Data Processing Paradigms}

Adopting streams requires a shift from batch-oriented thinking to event-driven, asynchronous processing models. This paradigm shift aligns perfectly with Node.js's non-blocking I/O philosophy and represents a more natural way to handle data as it flows through a system. By embracing this approach, developers can create more efficient data pipelines that mirror how information naturally moves through modern distributed systems.


\section{References}

\begin{itemize}
    \item Node.js. (2023). \textit{Stream | Node.js v18.x Documentation}. \href{https://nodejs.org/docs/latest-v18.x/api/stream.html}{Link}
    
    \item NodeSource. (2022). \textit{Understanding Streams in Node.js}. \href{https://nodesource.com/blog/understanding-streams-in-nodejs}{Link}
    
    \item Alapont, R. (2023). \textit{Streamlining Your Code: Best Practices for Node.js Streams}. \href{https://dev.to/ruben_alapont/streamlining-your-code-best-practices-for-nodejs-streams-1ji0}{Link}
    
    \item Alapont, R. (2023). \textit{Error Handling in Node.js Streams: Best Practices}. \href{https://dev.to/ruben_alapont/error-handling-in-nodejs-streams-best-practices-dhb}{Link}
    
    \item Clarion Technologies. (2022). \textit{Node.js for Real-Time Data Streaming}. \href{https://www.clariontech.com/blog/node.js-real-time-data-streaming}{Link}

    \item Translated, Edited and written in collaboration with AI.
\end{itemize}

\clearpage
\thispagestyle{empty}
\finalpagecontents

\end{document}